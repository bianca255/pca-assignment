{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4261a8bd",
   "metadata": {},
   "source": [
    "# FORMATIVE ASSIGNMENT: ADVANCED LINEAR ALGEBRA (PCA)\n",
    "\n",
    "## Principal Component Analysis Implementation\n",
    "\n",
    "**Student Name:** Haguma Bianca\n",
    "\n",
    "**Date:** February 2026\n",
    "\n",
    "\n",
    "\n",
    "## Assignment Overview\n",
    "This notebook implements Principal Component Analysis (PCA) from scratch using eigenvalue decomposition and covariance matrices. The goal is to reduce dimensionality while preserving maximum variance in the data.\n",
    "\n",
    "### Learning Objectives:\n",
    "1. Understand and implement covariance matrix calculation\n",
    "2. Perform eigendecomposition to find principal components\n",
    "3. Project data onto principal components\n",
    "4. Select components based on explained variance\n",
    "5. Visualize the transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d36a8c",
   "metadata": {},
   "source": [
    "## Part 0: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf84167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA as SklearnPCA\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43575b8a",
   "metadata": {},
   "source": [
    "## Part 1: Data Loading and Exploration\n",
    "\n",
    "### Data Requirements:\n",
    "- Must have missing values (NaN)\n",
    "- Must have at least 1 non-numeric column\n",
    "- Must have more than 10 columns\n",
    "- Should be impactful African/Africanized data\n",
    "\n",
    "### Example Dataset Suggestions:\n",
    "- African Economic Development Indicators\n",
    "- Healthcare/Disease Data from African Countries\n",
    "- Agricultural Production Data\n",
    "- Climate and Weather Data\n",
    "- Education Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133cfce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Load your dataset\n",
    "# Replace 'your_data.csv' with your actual data file\n",
    "# Example: df = pd.read_csv('african_health_data.csv')\n",
    "\n",
    "df = pd.read_csv('your_data.csv')\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nColumns ({len(df.columns)}):\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ad6b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61582600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about the dataset\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\"*50)\n",
    "df.info()\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nBasic Statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ee5c0c",
   "metadata": {},
   "source": [
    "## Part 2: Data Preprocessing\n",
    "\n",
    "### This section covers:\n",
    "1. Identifying missing values\n",
    "2. Identifying non-numeric columns\n",
    "3. Handling missing values\n",
    "4. Encoding categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00fb725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values Analysis:\")\n",
    "print(\"=\"*50)\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percent = (missing_values / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_values.index,\n",
    "    'Missing Count': missing_values.values,\n",
    "    'Percentage': missing_percent.values\n",
    "})\n",
    "\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "print(missing_df.to_string(index=False))\n",
    "\n",
    "# Visualize missing values\n",
    "if len(missing_df) > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(missing_df['Column'], missing_df['Percentage'])\n",
    "    plt.xlabel('Percentage of Missing Values')\n",
    "    plt.title('Missing Values by Column')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe1401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify non-numeric columns\n",
    "print(\"Data Types Analysis:\")\n",
    "print(\"=\"*50)\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "non_numeric_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"\\nNumeric columns ({len(numeric_cols)}):\")\n",
    "print(numeric_cols)\n",
    "print(f\"\\nNon-numeric columns ({len(non_numeric_cols)}):\")\n",
    "print(non_numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a9f0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Handle missing values\n",
    "# You can use various strategies:\n",
    "# 1. Mean/Median imputation for numeric columns\n",
    "# 2. Mode imputation for categorical columns\n",
    "# 3. Forward/Backward fill\n",
    "# 4. Drop rows/columns with too many missing values\n",
    "\n",
    "# Create a copy of the dataframe\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Strategy 1: For numeric columns, fill with median\n",
    "for col in numeric_cols:\n",
    "    if df_processed[col].isnull().any():\n",
    "        median_value = df_processed[col].median()\n",
    "        df_processed[col].fillna(median_value, inplace=True)\n",
    "        print(f\"Filled {col} with median: {median_value:.2f}\")\n",
    "\n",
    "# Strategy 2: For categorical columns, fill with mode\n",
    "for col in non_numeric_cols:\n",
    "    if df_processed[col].isnull().any():\n",
    "        mode_value = df_processed[col].mode()[0] if len(df_processed[col].mode()) > 0 else 'Unknown'\n",
    "        df_processed[col].fillna(mode_value, inplace=True)\n",
    "        print(f\"Filled {col} with mode: {mode_value}\")\n",
    "\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(df_processed.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36fa4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Encode non-numeric columns\n",
    "# Use Label Encoding or One-Hot Encoding depending on the nature of categorical variables\n",
    "\n",
    "df_encoded = df_processed.copy()\n",
    "\n",
    "# Label Encoding for non-numeric columns\n",
    "label_encoders = {}\n",
    "\n",
    "for col in non_numeric_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "    print(f\"Encoded {col}: {len(le.classes_)} unique categories\")\n",
    "\n",
    "print(\"\\nAll columns are now numeric!\")\n",
    "print(f\"Final shape: {df_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c863c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify no missing values and all numeric\n",
    "print(\"Final Data Check:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Missing values: {df_encoded.isnull().sum().sum()}\")\n",
    "print(f\"Non-numeric columns: {len(df_encoded.select_dtypes(exclude=[np.number]).columns)}\")\n",
    "print(f\"Total features: {df_encoded.shape[1]}\")\n",
    "print(f\"Total samples: {df_encoded.shape[0]}\")\n",
    "\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a01c09",
   "metadata": {},
   "source": [
    "## Part 3: Feature Standardization\n",
    "\n",
    "PCA is affected by the scale of features, so we need to standardize the data:\n",
    "- Mean = 0\n",
    "- Standard Deviation = 1\n",
    "\n",
    "Formula: $z = \\frac{x - \\mu}{\\sigma}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3781d9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Standardize the features\n",
    "# Use StandardScaler to normalize the data\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "X_scaled = scaler.fit_transform(df_encoded)\n",
    "\n",
    "print(\"Data standardized successfully!\")\n",
    "print(f\"Shape: {X_scaled.shape}\")\n",
    "print(f\"\\nMean of each feature (should be ~0):\")\n",
    "print(np.round(X_scaled.mean(axis=0), 6)[:5], \"...\")\n",
    "print(f\"\\nStandard deviation of each feature (should be ~1):\")\n",
    "print(np.round(X_scaled.std(axis=0), 6)[:5], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5013d7fc",
   "metadata": {},
   "source": [
    "## TASK 1: Implement PCA from Scratch\n",
    "\n",
    "### Steps:\n",
    "1. Calculate the covariance matrix\n",
    "2. Compute eigenvalues and eigenvectors\n",
    "3. Sort eigenvalues in descending order\n",
    "4. Select principal components\n",
    "5. Project data onto principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0066ad0d",
   "metadata": {},
   "source": [
    "### Step 1: Calculate Covariance Matrix\n",
    "\n",
    "The covariance matrix shows how features vary together:\n",
    "\n",
    "$\\text{Cov}(X, Y) = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})$\n",
    "\n",
    "For a matrix: $C = \\frac{1}{n-1} X^T X$ (when X is centered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe1c740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Calculate the covariance matrix\n",
    "# Hint: Use np.cov() with rowvar=False, or calculate manually using the formula\n",
    "\n",
    "# Method 1: Using NumPy\n",
    "covariance_matrix = np.cov(X_scaled, rowvar=False)\n",
    "\n",
    "# Method 2: Manual calculation (uncomment to use)\n",
    "# n_samples = X_scaled.shape[0]\n",
    "# covariance_matrix = (X_scaled.T @ X_scaled) / (n_samples - 1)\n",
    "\n",
    "print(\"Covariance Matrix:\")\n",
    "print(f\"Shape: {covariance_matrix.shape}\")\n",
    "print(f\"\\nFirst 5x5 elements:\")\n",
    "print(np.round(covariance_matrix[:5, :5], 4))\n",
    "\n",
    "# Visualize covariance matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(covariance_matrix, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Covariance Matrix Heatmap', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fbf869",
   "metadata": {},
   "source": [
    "### Step 2: Compute Eigenvalues and Eigenvectors\n",
    "\n",
    "Eigendecomposition: $Cv = \\lambda v$\n",
    "\n",
    "Where:\n",
    "- $C$ is the covariance matrix\n",
    "- $\\lambda$ are eigenvalues (variance along principal component)\n",
    "- $v$ are eigenvectors (direction of principal component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32870c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Compute eigenvalues and eigenvectors\n",
    "# Hint: Use np.linalg.eig() or np.linalg.eigh() for symmetric matrices\n",
    "\n",
    "# Calculate eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n",
    "\n",
    "print(\"Eigendecomposition completed!\")\n",
    "print(f\"Number of eigenvalues: {len(eigenvalues)}\")\n",
    "print(f\"Eigenvector matrix shape: {eigenvectors.shape}\")\n",
    "print(f\"\\nFirst 5 eigenvalues (before sorting):\")\n",
    "print(eigenvalues[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e99bda",
   "metadata": {},
   "source": [
    "### Step 3: Sort Eigenvalues and Eigenvectors in Descending Order\n",
    "\n",
    "Principal components are ordered by the amount of variance they explain.\n",
    "We need to sort eigenvalues from largest to smallest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7c62aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Sort eigenvalues and eigenvectors in descending order\n",
    "# Hint: Use np.argsort() with [::-1] to reverse the order\n",
    "\n",
    "# Get indices that would sort eigenvalues in descending order\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "\n",
    "# Sort eigenvalues and eigenvectors\n",
    "eigenvalues_sorted = eigenvalues[sorted_indices]\n",
    "eigenvectors_sorted = eigenvectors[:, sorted_indices]\n",
    "\n",
    "print(\"Eigenvalues and eigenvectors sorted!\")\n",
    "print(f\"\\nTop 10 eigenvalues (sorted):\")\n",
    "for i, ev in enumerate(eigenvalues_sorted[:10], 1):\n",
    "    print(f\"PC{i}: {ev:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f257ac92",
   "metadata": {},
   "source": [
    "### Step 4: Calculate Explained Variance Ratio\n",
    "\n",
    "Explained variance ratio shows how much information (variance) each principal component captures:\n",
    "\n",
    "$\\text{Explained Variance Ratio}_i = \\frac{\\lambda_i}{\\sum_{j=1}^{n} \\lambda_j}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4251d07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Calculate explained variance ratio for each principal component\n",
    "\n",
    "# Calculate explained variance ratio\n",
    "total_variance = np.sum(eigenvalues_sorted)\n",
    "explained_variance_ratio = eigenvalues_sorted / total_variance\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Create a detailed table\n",
    "variance_df = pd.DataFrame({\n",
    "    'PC': [f'PC{i+1}' for i in range(len(eigenvalues_sorted))],\n",
    "    'Eigenvalue': eigenvalues_sorted,\n",
    "    'Variance Ratio': explained_variance_ratio * 100,\n",
    "    'Cumulative Variance': cumulative_variance_ratio * 100\n",
    "})\n",
    "\n",
    "print(\"Explained Variance Analysis:\")\n",
    "print(\"=\"*70)\n",
    "print(variance_df.head(15).to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d374dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize explained variance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Scree plot\n",
    "axes[0].bar(range(1, min(21, len(eigenvalues_sorted)+1)), \n",
    "            explained_variance_ratio[:20] * 100, \n",
    "            alpha=0.7, color='steelblue')\n",
    "axes[0].set_xlabel('Principal Component', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Explained Variance (%)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Scree Plot: Variance Explained by Each PC', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Cumulative variance plot\n",
    "axes[1].plot(range(1, len(cumulative_variance_ratio)+1), \n",
    "             cumulative_variance_ratio * 100, \n",
    "             marker='o', linewidth=2, markersize=4, color='darkgreen')\n",
    "axes[1].axhline(y=95, color='r', linestyle='--', label='95% Variance', linewidth=2)\n",
    "axes[1].axhline(y=90, color='orange', linestyle='--', label='90% Variance', linewidth=2)\n",
    "axes[1].set_xlabel('Number of Components', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Cumulative Explained Variance (%)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Cumulative Variance Explained', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35598b6",
   "metadata": {},
   "source": [
    "## TASK 2: Dynamic Selection of Principal Components\n",
    "\n",
    "Select the number of components based on explained variance threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840383b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Implement dynamic component selection\n",
    "# Select components that explain at least 95% of variance\n",
    "\n",
    "variance_threshold = 0.95  # 95% variance retention\n",
    "\n",
    "# Find the number of components needed\n",
    "n_components = np.argmax(cumulative_variance_ratio >= variance_threshold) + 1\n",
    "\n",
    "print(f\"Dynamic Component Selection:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Variance threshold: {variance_threshold*100}%\")\n",
    "print(f\"Number of components selected: {n_components}\")\n",
    "print(f\"Total features in original dataset: {X_scaled.shape[1]}\")\n",
    "print(f\"Dimensionality reduction: {X_scaled.shape[1]} → {n_components}\")\n",
    "print(f\"Reduction percentage: {(1 - n_components/X_scaled.shape[1])*100:.2f}%\")\n",
    "print(f\"Actual variance explained: {cumulative_variance_ratio[n_components-1]*100:.2f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a626f106",
   "metadata": {},
   "source": [
    "### Step 5: Project Data onto Principal Components\n",
    "\n",
    "Transform the data to the new feature space:\n",
    "\n",
    "$X_{\\text{transformed}} = X \\cdot W$\n",
    "\n",
    "Where $W$ is the matrix of selected eigenvectors (principal components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3709065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Project the data onto the selected principal components\n",
    "\n",
    "# Select the top n_components eigenvectors\n",
    "principal_components = eigenvectors_sorted[:, :n_components]\n",
    "\n",
    "print(f\"Principal components matrix shape: {principal_components.shape}\")\n",
    "\n",
    "# Project the data\n",
    "X_pca = X_scaled @ principal_components\n",
    "\n",
    "print(f\"Transformed data shape: {X_pca.shape}\")\n",
    "print(f\"\\nOriginal shape: {X_scaled.shape}\")\n",
    "print(f\"Reduced shape: {X_pca.shape}\")\n",
    "print(f\"\\nFirst 5 samples in PC space:\")\n",
    "print(X_pca[:5, :min(5, n_components)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca9220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with PC values\n",
    "pc_columns = [f'PC{i+1}' for i in range(n_components)]\n",
    "df_pca = pd.DataFrame(X_pca, columns=pc_columns)\n",
    "\n",
    "print(\"PCA DataFrame created!\")\n",
    "print(f\"\\nStatistics of Principal Components:\")\n",
    "df_pca.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5996bb",
   "metadata": {},
   "source": [
    "## TASK 3: Visualization - Before and After PCA\n",
    "\n",
    "Compare the original feature space with the transformed principal component space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9493b1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Create visualizations comparing before and after PCA\n",
    "\n",
    "# Select two features from original data for visualization\n",
    "feature_1_idx = 0\n",
    "feature_2_idx = 1\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# BEFORE PCA: Original feature space\n",
    "axes[0].scatter(X_scaled[:, feature_1_idx], X_scaled[:, feature_2_idx], \n",
    "                alpha=0.6, s=50, c='steelblue', edgecolors='black', linewidth=0.5)\n",
    "axes[0].set_xlabel(f'Feature {feature_1_idx+1}: {df_encoded.columns[feature_1_idx]}', \n",
    "                   fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel(f'Feature {feature_2_idx+1}: {df_encoded.columns[feature_2_idx]}', \n",
    "                   fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('BEFORE PCA: Original Feature Space', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].axhline(y=0, color='k', linewidth=0.5)\n",
    "axes[0].axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "# AFTER PCA: Principal component space\n",
    "axes[1].scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                alpha=0.6, s=50, c='darkgreen', edgecolors='black', linewidth=0.5)\n",
    "axes[1].set_xlabel(f'PC1 ({explained_variance_ratio[0]*100:.2f}% variance)', \n",
    "                   fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel(f'PC2 ({explained_variance_ratio[1]*100:.2f}% variance)', \n",
    "                   fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('AFTER PCA: Principal Component Space', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].axhline(y=0, color='k', linewidth=0.5)\n",
    "axes[1].axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization Analysis:\")\n",
    "print(\"=\"*70)\n",
    "print(\"BEFORE PCA:\")\n",
    "print(f\"  - Shows relationship between two original features\")\n",
    "print(f\"  - Data may be correlated and have redundant information\")\n",
    "print(f\"\\nAFTER PCA:\")\n",
    "print(f\"  - PC1 captures {explained_variance_ratio[0]*100:.2f}% of total variance\")\n",
    "print(f\"  - PC2 captures {explained_variance_ratio[1]*100:.2f}% of total variance\")\n",
    "print(f\"  - PCs are uncorrelated (orthogonal to each other)\")\n",
    "print(f\"  - Data is rotated to align with directions of maximum variance\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690be9bf",
   "metadata": {},
   "source": [
    "### Interpretation of PCA Transformation:\n",
    "\n",
    "**TO DO: Write your interpretation here**\n",
    "\n",
    "1. **Data Structure Preservation:**\n",
    "   - [Explain how the overall structure of data points is preserved]\n",
    "   - [Discuss whether clusters or patterns remain visible]\n",
    "\n",
    "2. **Variance Explanation:**\n",
    "   - [Explain what PC1 and PC2 represent]\n",
    "   - [Discuss how much variance is captured by these two components]\n",
    "\n",
    "3. **Transformation Effects:**\n",
    "   - [Describe how PCA has rotated/transformed the data]\n",
    "   - [Explain the benefits of this transformation for your specific dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e28ffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional visualization: 3D plot if you have at least 3 PCs\n",
    "if n_components >= 3:\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    \n",
    "    fig = plt.figure(figsize=(14, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2],\n",
    "                        c=X_pca[:, 0], cmap='viridis', \n",
    "                        s=50, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    ax.set_xlabel(f'PC1 ({explained_variance_ratio[0]*100:.2f}%)', fontweight='bold')\n",
    "    ax.set_ylabel(f'PC2 ({explained_variance_ratio[1]*100:.2f}%)', fontweight='bold')\n",
    "    ax.set_zlabel(f'PC3 ({explained_variance_ratio[2]*100:.2f}%)', fontweight='bold')\n",
    "    ax.set_title('3D Visualization of First 3 Principal Components', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.colorbar(scatter, ax=ax, label='PC1 Value')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"First 3 PCs explain {cumulative_variance_ratio[2]*100:.2f}% of total variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec688b51",
   "metadata": {},
   "source": [
    "## TASK 4: Validation and Comparison with Scikit-learn\n",
    "\n",
    "Verify your implementation by comparing with scikit-learn's PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b94a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with scikit-learn implementation\n",
    "print(\"Validation: Comparing with Scikit-learn PCA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Apply scikit-learn PCA\n",
    "sklearn_pca = SklearnPCA(n_components=n_components)\n",
    "X_pca_sklearn = sklearn_pca.fit_transform(X_scaled)\n",
    "\n",
    "# Compare explained variance ratios\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Component': [f'PC{i+1}' for i in range(n_components)],\n",
    "    'Our Implementation': explained_variance_ratio[:n_components] * 100,\n",
    "    'Scikit-learn': sklearn_pca.explained_variance_ratio_ * 100,\n",
    "    'Difference': np.abs(explained_variance_ratio[:n_components] - \n",
    "                        sklearn_pca.explained_variance_ratio_) * 100\n",
    "})\n",
    "\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Check if implementations match (allowing for sign differences)\n",
    "max_difference = comparison_df['Difference'].max()\n",
    "if max_difference < 0.01:\n",
    "    print(\"✓ Implementation is CORRECT! Matches scikit-learn.\")\n",
    "else:\n",
    "    print(f\"⚠ Small differences detected (max: {max_difference:.6f}%)\")\n",
    "    print(\"  This is normal due to numerical precision and sign ambiguity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd1c9c6",
   "metadata": {},
   "source": [
    "## TASK 5: Performance Benchmarking\n",
    "\n",
    "Measure the performance of your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a91c84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Benchmark your PCA implementation\n",
    "\n",
    "print(\"Performance Benchmarking\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Benchmark custom implementation\n",
    "start_time = time.time()\n",
    "\n",
    "# Repeat the PCA steps\n",
    "cov_matrix = np.cov(X_scaled, rowvar=False)\n",
    "eigenvals, eigenvecs = np.linalg.eigh(cov_matrix)\n",
    "sorted_idx = np.argsort(eigenvals)[::-1]\n",
    "eigenvals_sorted = eigenvals[sorted_idx]\n",
    "eigenvecs_sorted = eigenvecs[:, sorted_idx]\n",
    "pcs = eigenvecs_sorted[:, :n_components]\n",
    "X_transformed = X_scaled @ pcs\n",
    "\n",
    "custom_time = time.time() - start_time\n",
    "\n",
    "# Benchmark scikit-learn\n",
    "start_time = time.time()\n",
    "sklearn_pca = SklearnPCA(n_components=n_components)\n",
    "X_sklearn = sklearn_pca.fit_transform(X_scaled)\n",
    "sklearn_time = time.time() - start_time\n",
    "\n",
    "print(f\"Custom Implementation Time: {custom_time:.6f} seconds\")\n",
    "print(f\"Scikit-learn Time: {sklearn_time:.6f} seconds\")\n",
    "print(f\"\\nSpeed Comparison:\")\n",
    "if custom_time < sklearn_time:\n",
    "    print(f\"  Custom is {sklearn_time/custom_time:.2f}x FASTER\")\n",
    "else:\n",
    "    print(f\"  Scikit-learn is {custom_time/sklearn_time:.2f}x faster\")\n",
    "    print(f\"  (This is expected as scikit-learn is highly optimized)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3112f8cf",
   "metadata": {},
   "source": [
    "## Part 4: Analysis and Insights\n",
    "\n",
    "### Feature Importance in Principal Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d812cdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature contributions to principal components\n",
    "n_display_pcs = min(5, n_components)\n",
    "n_top_features = 10\n",
    "\n",
    "# Create loadings matrix (correlation between original features and PCs)\n",
    "loadings = eigenvectors_sorted[:, :n_display_pcs]\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "loadings_df = pd.DataFrame(\n",
    "    loadings,\n",
    "    columns=[f'PC{i+1}' for i in range(n_display_pcs)],\n",
    "    index=df_encoded.columns\n",
    ")\n",
    "\n",
    "print(\"Feature Loadings (Contributions to Principal Components):\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Show top contributing features for each PC\n",
    "for i in range(n_display_pcs):\n",
    "    pc_name = f'PC{i+1}'\n",
    "    print(f\"\\n{pc_name} (explains {explained_variance_ratio[i]*100:.2f}% variance):\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Get top positive and negative loadings\n",
    "    top_features = loadings_df[pc_name].abs().nlargest(n_top_features)\n",
    "    \n",
    "    for feature, loading in top_features.items():\n",
    "        actual_loading = loadings_df.loc[feature, pc_name]\n",
    "        print(f\"  {feature:30s}: {actual_loading:7.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e6ad59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize loadings as a heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(loadings_df.head(20), cmap='RdBu_r', center=0, \n",
    "            annot=True, fmt='.2f', cbar_kws={'label': 'Loading'})\n",
    "plt.title('Feature Loadings on Principal Components\\n(First 20 features)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Principal Component', fontweight='bold')\n",
    "plt.ylabel('Original Feature', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
